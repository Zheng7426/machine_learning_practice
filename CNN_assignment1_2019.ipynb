{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "CNN_assignment1_2019.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zheng7426/machine_learning_practice/blob/master/CNN_assignment1_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QS1r6WpVVia",
        "colab_type": "text"
      },
      "source": [
        "Upload The cifar10 from the keras.datasets. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, \n",
        "with 6000 images per class. There are 50000 training images and 10000 test images. \n",
        "\n",
        "for more information about the dataset please see: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "\n",
        "Split data to the train and test.\n",
        "Convert the class label to binary class label\n",
        "## Assignment 3\n",
        "### Zheng Huang 001430398"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pRj35GAX32s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.layers import Input,Activation,Dropout,LeakyReLU,AveragePooling2D,Flatten,Dense,Conv2D,BatchNormalization,MaxPooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "import keras.backend as K\n",
        "\n",
        "from keras.datasets import cifar10\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf73Cc2qVVid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the dataset\n",
        "num_classes = 10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKNawxNNfJa6",
        "colab_type": "code",
        "outputId": "e34d12dd-3f99-49fb-a021-5650b92cf4ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_XcNpGmVVij",
        "colab_type": "text"
      },
      "source": [
        "Normalized the data and train the model using CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miLge_1wduHl",
        "colab_type": "text"
      },
      "source": [
        "Pixel values for each image in the dataset are integers in the range between 0 and 255(no color and full color).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZUxTL8NVVik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Rescale the pixel values to the range[0,1]\n",
        "# Convert from integer to floats\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Converts a class vector (integers) to binary class matrix, \n",
        "# transforming the class interger to into a 10-element binary vector with a 1 for the index of the class\n",
        "y_train = to_categorical(y_train, num_classes)\n",
        "y_test = to_categorical(y_test, num_classes)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rrqIu0zkbVk",
        "colab_type": "text"
      },
      "source": [
        "Model structure:<br>\n",
        "BatchNormalization is a technique which subtracts the weights from its mean and then divide by standard deviation. It prevents the weights to go off the rails. <br>\n",
        "LeakyReLU is to add non-linearity to the system.<br>\n",
        "MaxPooling2D is to reduce the dimension of original layer by selecting the max values in an area of data.<br>\n",
        "Dropout prevents overfitting.<br>\n",
        "After flatten layer, the Dense layer fully connects all the weights with each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-t6JlYRivNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_layer = Input((32,32,3))\n",
        "\n",
        "x = Conv2D(filters = 32, kernel_size = 3, strides = 1, padding = 'same')(input_layer)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU()(x)\n",
        "\n",
        "\n",
        "x = Conv2D(filters = 32, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
        "x = BatchNormalization(momentum=0.9)(x)\n",
        "x = LeakyReLU()(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "x = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = LeakyReLU()(x)\n",
        "\n",
        "\n",
        "x = Conv2D(filters = 64, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
        "x = BatchNormalization(momentum=0.9)(x)\n",
        "x = LeakyReLU()(x)\n",
        "x = MaxPooling2D((2, 2))(x)\n",
        "x = Dropout(rate = 0.2)(x)\n",
        "\n",
        "\n",
        "\n",
        "x = Conv2D(filters = 82, kernel_size = 3, strides = 1, padding = 'same')(x)\n",
        "x = BatchNormalization(momentum=0.9)(x)\n",
        "x = LeakyReLU()(x)\n",
        "\n",
        "\n",
        "\n",
        "x = Conv2D(filters = 82, kernel_size = 3, strides = 2, padding = 'same')(x)\n",
        "x = BatchNormalization(momentum=0.9)(x)\n",
        "x = LeakyReLU()(x)\n",
        "\n",
        "\n",
        "\n",
        "x = Flatten()(x)\n",
        "\n",
        "x = Dense(128)(x)\n",
        "x = LeakyReLU()(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(rate = 0.5)(x)\n",
        "\n",
        "x = Dense(num_classes)(x)\n",
        "output_layer = Activation('softmax')(x)\n",
        "\n",
        "model = Model(input_layer, output_layer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwwke4XPj4ji",
        "colab_type": "code",
        "outputId": "0b4cda16-9b2a-4240-9514-a9a684513bcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_11 (InputLayer)        (None, 32, 32, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_57 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_66 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_66 (LeakyReLU)   (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_58 (Conv2D)           (None, 16, 16, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_67 (Batc (None, 16, 16, 32)        128       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_67 (LeakyReLU)   (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_59 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_68 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_68 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_60 (Conv2D)           (None, 8, 8, 64)          36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_69 (Batc (None, 8, 8, 64)          256       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_69 (LeakyReLU)   (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 4, 4, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_61 (Conv2D)           (None, 4, 4, 82)          47314     \n",
            "_________________________________________________________________\n",
            "batch_normalization_70 (Batc (None, 4, 4, 82)          328       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_70 (LeakyReLU)   (None, 4, 4, 82)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_62 (Conv2D)           (None, 2, 2, 82)          60598     \n",
            "_________________________________________________________________\n",
            "batch_normalization_71 (Batc (None, 2, 2, 82)          328       \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_71 (LeakyReLU)   (None, 2, 2, 82)          0         \n",
            "_________________________________________________________________\n",
            "flatten_10 (Flatten)         (None, 328)               0         \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 128)               42112     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_72 (LeakyReLU)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_72 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 10)                1290      \n",
            "_________________________________________________________________\n",
            "activation_10 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 218,818\n",
            "Trainable params: 217,850\n",
            "Non-trainable params: 968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NCED0Rej9pS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = Adam(lr=0.001)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc3z-CfJkIse",
        "colab_type": "code",
        "outputId": "0ac41539-b7de-43aa-8f80-c3b4b6211330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        }
      },
      "source": [
        "model.fit(x_train\n",
        "         , y_train\n",
        "         , batch_size=32\n",
        "         , epochs=20\n",
        "         , shuffle=True\n",
        "         , validation_data = (x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 48s 958us/step - loss: 1.6241 - acc: 0.4414 - val_loss: 1.1925 - val_acc: 0.5666\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 42s 835us/step - loss: 1.1567 - acc: 0.5887 - val_loss: 1.0365 - val_acc: 0.6295\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 43s 851us/step - loss: 1.0069 - acc: 0.6472 - val_loss: 0.9750 - val_acc: 0.6557\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 42s 834us/step - loss: 0.9132 - acc: 0.6806 - val_loss: 0.8576 - val_acc: 0.7067\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 41s 827us/step - loss: 0.8448 - acc: 0.7070 - val_loss: 0.8183 - val_acc: 0.7177\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 42s 840us/step - loss: 0.7836 - acc: 0.7269 - val_loss: 0.8196 - val_acc: 0.7130\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 42s 847us/step - loss: 0.7340 - acc: 0.7473 - val_loss: 0.7338 - val_acc: 0.7445\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 42s 831us/step - loss: 0.7026 - acc: 0.7584 - val_loss: 0.7380 - val_acc: 0.7455\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 42s 831us/step - loss: 0.6627 - acc: 0.7728 - val_loss: 0.7430 - val_acc: 0.7478\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 42s 830us/step - loss: 0.6354 - acc: 0.7792 - val_loss: 0.7150 - val_acc: 0.7532\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 42s 834us/step - loss: 0.6046 - acc: 0.7922 - val_loss: 0.7151 - val_acc: 0.7497\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 41s 830us/step - loss: 0.5810 - acc: 0.8004 - val_loss: 0.6286 - val_acc: 0.7821\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 41s 830us/step - loss: 0.5540 - acc: 0.8110 - val_loss: 0.6552 - val_acc: 0.7794\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 42s 831us/step - loss: 0.5366 - acc: 0.8162 - val_loss: 0.6626 - val_acc: 0.7731\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 42s 831us/step - loss: 0.5165 - acc: 0.8231 - val_loss: 0.6344 - val_acc: 0.7851\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 41s 830us/step - loss: 0.5034 - acc: 0.8256 - val_loss: 0.6495 - val_acc: 0.7778\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 42s 830us/step - loss: 0.4787 - acc: 0.8348 - val_loss: 0.6118 - val_acc: 0.7947\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 42s 833us/step - loss: 0.4625 - acc: 0.8407 - val_loss: 0.6175 - val_acc: 0.7899\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 42s 831us/step - loss: 0.4544 - acc: 0.8435 - val_loss: 0.6288 - val_acc: 0.7883\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 42s 833us/step - loss: 0.4385 - acc: 0.8487 - val_loss: 0.6570 - val_acc: 0.7861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0ee05755f8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJkZoqRvVVip",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the accuracy of the model. The first value is for loss. The second value returned is accuracy of the model regarding test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elOZKKstVViq",
        "colab_type": "code",
        "outputId": "209c6404-3a33-4b4c-bd35-04fcdcefa1e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(x_test, y_test, batch_size=1000)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 83us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6569806158542633, 0.7861000001430511]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    }
  ]
}